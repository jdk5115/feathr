{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "feathr12345syws"
		},
		"feathr12345syws-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'feathr12345syws-WorkspaceDefaultSqlServer'"
		},
		"feathr12345syws-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://feathr12345dls.dfs.core.windows.net/"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/feathr12345syws-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('feathr12345syws-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/feathr12345syws-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('feathr12345syws-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/feathr_demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "487ba5a5-dd79-4cf4-88ef-3175bf106b58"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/d27c6b88-6870-4df2-8b38-43c16f0f9d52/resourceGroups/rg-jonathan/providers/Microsoft.Synapse/workspaces/feathr12345syws/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://feathr12345syws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"resource_prefix = \"feathr12345\""
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"pip install feathr azure-cli  pandavro scikit-learn"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"az login --use-device-code"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pkg_resources\r\n",
							"\r\n",
							"installed_packages = pkg_resources.working_set\r\n",
							"installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\r\n",
							"   for i in installed_packages])\r\n",
							"\r\n",
							"print(installed_packages_list)"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import glob\n",
							"import os\n",
							"import tempfile\n",
							"from datetime import datetime, timedelta\n",
							"from math import sqrt\n",
							"print(10)\n",
							"import pandas as pd\n",
							"import pandavro as pdx\n",
							"\n",
							"print(10)\n",
							"from feathr import FeathrClient\n",
							"print(10)\n",
							"from feathr import BOOLEAN, FLOAT, INT32, ValueType\n",
							"from feathr import Feature, DerivedFeature, FeatureAnchor\n",
							"from feathr import BackfillTime, MaterializationSettings\n",
							"from feathr import FeatureQuery, ObservationSettings\n",
							"from feathr import RedisSink\n",
							"from feathr import INPUT_CONTEXT, HdfsSource\n",
							"from feathr import WindowAggTransformation\n",
							"from feathr import TypedKey\n",
							"from sklearn.metrics import mean_squared_error\n",
							"from sklearn.model_selection import train_test_split\n",
							"from azure.identity import DefaultAzureCredential\n",
							"from azure.keyvault.secrets import SecretClient"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Get all the required credentials from Azure Key Vault\n",
							"key_vault_name=resource_prefix+\"kv\"\n",
							"synapse_workspace_url=resource_prefix+\"syws\"\n",
							"adls_account=resource_prefix+\"dls\"\n",
							"adls_fs_name=resource_prefix+\"fs\"\n",
							"purview_name=resource_prefix+\"purview\"\n",
							"key_vault_uri = f\"https://{key_vault_name}.vault.azure.net\"\n",
							"credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
							"client = SecretClient(vault_url=key_vault_uri, credential=credential)\n",
							"secretName = \"FEATHR-ONLINE-STORE-CONN\"\n",
							"retrieved_secret = client.get_secret(secretName).value\n",
							"\n",
							"# Get redis credentials; This is to parse Redis connection string.\n",
							"redis_port=retrieved_secret.split(',')[0].split(\":\")[1]\n",
							"redis_host=retrieved_secret.split(',')[0].split(\":\")[0]\n",
							"redis_password=retrieved_secret.split(',')[1].split(\"password=\",1)[1]\n",
							"redis_ssl=retrieved_secret.split(',')[2].split(\"ssl=\",1)[1]\n",
							"\n",
							"# Set the resource link\n",
							"os.environ['spark_config__azure_synapse__dev_url'] = f'https://{synapse_workspace_url}.dev.azuresynapse.net'\n",
							"os.environ['spark_config__azure_synapse__pool_name'] = 'spark31'\n",
							"os.environ['spark_config__azure_synapse__workspace_dir'] = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_project'\n",
							"os.environ['feature_registry__purview__purview_name'] = f'{purview_name}'\n",
							"os.environ['online_store__redis__host'] = redis_host\n",
							"os.environ['online_store__redis__port'] = redis_port\n",
							"os.environ['online_store__redis__ssl_enabled'] = redis_ssl\n",
							"os.environ['REDIS_PASSWORD']=redis_password\n",
							"os.environ['feature_registry__purview__purview_name'] = f'{purview_name}'\n",
							"feathr_output_path = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_output'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import tempfile\n",
							"yaml_config = \"\"\"\n",
							"# Please refer to https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml for explanations on the meaning of each field.\n",
							"api_version: 1\n",
							"project_config:\n",
							"  project_name: 'feathr_getting_started'\n",
							"  required_environment_variables:\n",
							"    - 'REDIS_PASSWORD'\n",
							"    - 'AZURE_CLIENT_ID'\n",
							"    - 'AZURE_TENANT_ID'\n",
							"    - 'AZURE_CLIENT_SECRET'\n",
							"offline_store:\n",
							"  adls:\n",
							"    adls_enabled: true\n",
							"  wasb:\n",
							"    wasb_enabled: true\n",
							"  s3:\n",
							"    s3_enabled: false\n",
							"    s3_endpoint: 's3.amazonaws.com'\n",
							"  jdbc:\n",
							"    jdbc_enabled: false\n",
							"    jdbc_database: 'feathrtestdb'\n",
							"    jdbc_table: 'feathrtesttable'\n",
							"  snowflake:\n",
							"    url: \"dqllago-ol19457.snowflakecomputing.com\"\n",
							"    user: \"feathrintegration\"\n",
							"    role: \"ACCOUNTADMIN\"\n",
							"spark_config:\n",
							"  spark_cluster: 'azure_synapse'\n",
							"  spark_result_output_parts: '1'\n",
							"  azure_synapse:\n",
							"    dev_url: 'https://feathrazuretest3synapse.dev.azuresynapse.net'\n",
							"    pool_name: 'spark3'\n",
							"    workspace_dir: 'abfss://feathrazuretest3fs@feathrazuretest3storage.dfs.core.windows.net/feathr_getting_started'\n",
							"    executor_size: 'Small'\n",
							"    executor_num: 4\n",
							"    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
							"  databricks:\n",
							"    workspace_instance_url: 'https://adb-2474129336842816.16.azuredatabricks.net'\n",
							"    config_template: {'run_name':'','new_cluster':{'spark_version':'9.1.x-scala2.12','node_type_id':'Standard_D3_v2','num_workers':2,'spark_conf':{}},'libraries':[{'jar':''}],'spark_jar_task':{'main_class_name':'','parameters':['']}}\n",
							"    work_dir: 'dbfs:/feathr_getting_started'\n",
							"    feathr_runtime_location: https://azurefeathrstorage.blob.core.windows.net/public/feathr-assembly-LATEST.jar\n",
							"online_store:\n",
							"  redis:\n",
							"    host: 'feathrazuretest3redis.redis.cache.windows.net'\n",
							"    port: 6380\n",
							"    ssl_enabled: True\n",
							"feature_registry:\n",
							"  purview:\n",
							"    type_system_initialization: true\n",
							"    purview_name: 'feathrazuretest3-purview1'\n",
							"    delimiter: '__'\n",
							"\"\"\"\n",
							"tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
							"with open(tmp.name, \"w\") as text_file:\n",
							"    text_file.write(yaml_config)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# os.environ['REDIS_PASSWORD'] = ''\n",
							"# os.environ['AZURE_CLIENT_ID'] = ''\n",
							"# os.environ['AZURE_TENANT_ID'] = ''\n",
							"# os.environ['AZURE_CLIENT_SECRET'] = ''\n",
							"\n",
							"# # Optional envs if you are using different runtimes\n",
							"# os.environ['DATABRICKS_WORKSPACE_TOKEN_VALUE'] = ''"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"client = FeathrClient(config_path=tmp.name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pandas as pd\n",
							"pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/green_tripdata_2020-04_with_index.csv\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import SparkSession, DataFrame\n",
							"def feathr_udf_day_calc(df: DataFrame) -> DataFrame:\n",
							"    from pyspark.sql.functions import dayofweek, dayofyear, col\n",
							"    df = df.withColumn(\"fare_amount_cents\", col(\"fare_amount\")*100)\n",
							"    return df\n",
							"\n",
							"batch_source = HdfsSource(name=\"nycTaxiBatchSource\",\n",
							"                          path=\"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/green_tripdata_2020-04_with_index.csv\",\n",
							"                          event_timestamp_column=\"lpep_dropoff_datetime\",\n",
							"                          preprocessing=feathr_udf_day_calc,\n",
							"                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"f_trip_distance = Feature(name=\"f_trip_distance\",\n",
							"                          feature_type=FLOAT, transform=\"trip_distance\")\n",
							"f_trip_time_duration = Feature(name=\"f_trip_time_duration\",\n",
							"                               feature_type=INT32,\n",
							"                               transform=\"(to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime))/60\")\n",
							"\n",
							"features = [\n",
							"    f_trip_distance,\n",
							"    f_trip_time_duration,\n",
							"    Feature(name=\"f_is_long_trip_distance\",\n",
							"            feature_type=BOOLEAN,\n",
							"            transform=\"cast_float(trip_distance)>30\"),\n",
							"    Feature(name=\"f_day_of_week\",\n",
							"            feature_type=INT32,\n",
							"            transform=\"dayofweek(lpep_dropoff_datetime)\"),\n",
							"]\n",
							"\n",
							"request_anchor = FeatureAnchor(name=\"request_features\",\n",
							"                               source=INPUT_CONTEXT,\n",
							"                               features=features)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"location_id = TypedKey(key_column=\"DOLocationID\",\n",
							"                       key_column_type=ValueType.INT32,\n",
							"                       description=\"location id in NYC\",\n",
							"                       full_name=\"nyc_taxi.location_id\")\n",
							"agg_features = [Feature(name=\"f_location_avg_fare\",\n",
							"                        key=location_id,\n",
							"                        feature_type=FLOAT,\n",
							"                        transform=WindowAggTransformation(agg_expr=\"cast_float(fare_amount)\",\n",
							"                                                          agg_func=\"AVG\",\n",
							"                                                          window=\"90d\")),\n",
							"                Feature(name=\"f_location_max_fare\",\n",
							"                        key=location_id,\n",
							"                        feature_type=FLOAT,\n",
							"                        transform=WindowAggTransformation(agg_expr=\"cast_float(fare_amount)\",\n",
							"                                                          agg_func=\"MAX\",\n",
							"                                                          window=\"90d\")),\n",
							"                Feature(name=\"f_location_total_fare_cents\",\n",
							"                        key=location_id,\n",
							"                        feature_type=FLOAT,\n",
							"                        transform=WindowAggTransformation(agg_expr=\"fare_amount_cents\",\n",
							"                                                          agg_func=\"SUM\",\n",
							"                                                          window=\"90d\")),\n",
							"                ]\n",
							"\n",
							"agg_anchor = FeatureAnchor(name=\"aggregationFeatures\",\n",
							"                           source=batch_source,\n",
							"                           features=agg_features)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"f_trip_time_distance = DerivedFeature(name=\"f_trip_time_distance\",\n",
							"                                      feature_type=FLOAT,\n",
							"                                      input_features=[\n",
							"                                          f_trip_distance, f_trip_time_duration],\n",
							"                                      transform=\"f_trip_distance * f_trip_time_duration\")\n",
							"\n",
							"f_trip_time_rounded = DerivedFeature(name=\"f_trip_time_rounded\",\n",
							"                                     feature_type=INT32,\n",
							"                                     input_features=[f_trip_time_duration],\n",
							"                                     transform=\"f_trip_time_duration % 10\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"client.build_features(anchor_list=[agg_anchor, request_anchor], derived_feature_list=[\n",
							"                      f_trip_time_distance, f_trip_time_rounded])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"if client.spark_runtime == 'databricks':\n",
							"    output_path = 'dbfs:/feathrazure_test.avro'\n",
							"else:\n",
							"    output_path = feathr_output_path\n",
							"\n",
							"\n",
							"feature_query = FeatureQuery(\n",
							"    feature_list=[\"f_location_avg_fare\", \"f_trip_time_rounded\", \"f_is_long_trip_distance\", \"f_location_total_fare_cents\"], key=location_id)\n",
							"settings = ObservationSettings(\n",
							"    observation_path=\"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/green_tripdata_2020-04_with_index.csv\",\n",
							"    event_timestamp_column=\"lpep_dropoff_datetime\",\n",
							"    timestamp_format=\"yyyy-MM-dd HH:mm:ss\")\n",
							"client.get_offline_features(observation_settings=settings,\n",
							"                            feature_query=feature_query,\n",
							"                            output_path=output_path)\n",
							"client.wait_job_to_finish(timeout_sec=500)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
							"    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
							"    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
							"    tmp_dir = tempfile.TemporaryDirectory()\n",
							"    client.feathr_spark_laucher.download_result(result_path=res_url, local_folder=tmp_dir.name)\n",
							"    dataframe_list = []\n",
							"    # assuming the result are in avro format\n",
							"    for file in glob.glob(os.path.join(tmp_dir.name, '*.avro')):\n",
							"        dataframe_list.append(pdx.read_avro(file))\n",
							"    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
							"    tmp_dir.cleanup()\n",
							"    return vertical_concat_df\n",
							"\n",
							"df_res = get_result_df(client)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"df_res"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# remove columns\n",
							"from sklearn.ensemble import GradientBoostingRegressor\n",
							"final_df = df_res\n",
							"final_df.drop([\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\",\n",
							"              \"store_and_fwd_flag\"], axis=1, inplace=True, errors='ignore')\n",
							"final_df.fillna(0, inplace=True)\n",
							"final_df['fare_amount'] = final_df['fare_amount'].astype(\"float64\")\n",
							"\n",
							"\n",
							"train_x, test_x, train_y, test_y = train_test_split(final_df.drop([\"fare_amount\"], axis=1),\n",
							"                                                    final_df[\"fare_amount\"],\n",
							"                                                    test_size=0.2,\n",
							"                                                    random_state=42)\n",
							"model = GradientBoostingRegressor()\n",
							"model.fit(train_x, train_y)\n",
							"\n",
							"y_predict = model.predict(test_x)\n",
							"\n",
							"y_actual = test_y.values.flatten().tolist()\n",
							"rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
							"\n",
							"sum_actuals = sum_errors = 0\n",
							"\n",
							"for actual_val, predict_val in zip(y_actual, y_predict):\n",
							"    abs_error = actual_val - predict_val\n",
							"    if abs_error < 0:\n",
							"        abs_error = abs_error * -1\n",
							"\n",
							"    sum_errors = sum_errors + abs_error\n",
							"    sum_actuals = sum_actuals + actual_val\n",
							"\n",
							"mean_abs_percent_error = sum_errors / sum_actuals\n",
							"print(\"Model MAPE:\")\n",
							"print(mean_abs_percent_error)\n",
							"print()\n",
							"print(\"Model Accuracy:\")\n",
							"print(1 - mean_abs_percent_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"backfill_time = BackfillTime(start=datetime(\n",
							"    2020, 5, 20), end=datetime(2020, 5, 20), step=timedelta(days=1))\n",
							"redisSink = RedisSink(table_name=\"nycTaxiDemoFeature\")\n",
							"settings = MaterializationSettings(\"nycTaxiTable\",\n",
							"                                   backfill_time=backfill_time,\n",
							"                                   sinks=[redisSink],\n",
							"                                   feature_names=[\"f_location_avg_fare\", \"f_location_max_fare\"])\n",
							"\n",
							"client.materialize_features(settings)\n",
							"client.wait_job_to_finish(timeout_sec=500)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"res = client.get_online_features('nycTaxiDemoFeature', '265', [\n",
							"                                 'f_location_avg_fare', 'f_location_max_fare'])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"client.multi_get_online_features(\"nycTaxiDemoFeature\", [\"239\", \"265\"], [\n",
							"                                 'f_location_avg_fare', 'f_location_max_fare'])"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 30
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"libraryRequirements": {
					"content": "feathr==0.4.0\r\nazure-cli==2.37.0\r\npandavro==1.6.0\r\nscikit-learn==1.1.1\r\naiohttp==3.8.0",
					"filename": "requirements.txt",
					"time": "2022-05-26T20:15:23.9913386Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sql--feathr12345syws')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/d27c6b88-6870-4df2-8b38-43c16f0f9d52/resourceGroups/rg-jonathan/providers/Microsoft.Synapse/workspaces/feathr12345syws",
				"groupId": "sql",
				"fqdns": [
					"feathr12345syws.d3a46b4e-b575-41c9-9364-3adbf44c04e1.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default/synapse-ws-sqlOnDemand--feathr12345syws')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks/managedPrivateEndpoints",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"privateLinkResourceId": "/subscriptions/d27c6b88-6870-4df2-8b38-43c16f0f9d52/resourceGroups/rg-jonathan/providers/Microsoft.Synapse/workspaces/feathr12345syws",
				"groupId": "sqlOnDemand",
				"fqdns": [
					"feathr12345syws-ondemand.d3a46b4e-b575-41c9-9364-3adbf44c04e1.sql.azuresynapse.net"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		}
	]
}